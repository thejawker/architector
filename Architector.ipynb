{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Architector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhEDeJcPHeMyAqqscKqD+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thejawker/architector/blob/master/Architector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyT8erRV4NoV",
        "colab_type": "text"
      },
      "source": [
        "# Architector\n",
        "\n",
        "> Algorithm to detect features of residential architecture.\n",
        "\n",
        "\n",
        "## Possible Pipeline\n",
        "1. Detect if image has house\n",
        "2. Do a rough detection on labeled architecture\n",
        "  - a. find a large dataset of house fronts\n",
        "  - b. label these with the right architecture\n",
        "  - c. train model\n",
        "  - d. win\n",
        "3. Recognise features (roof, window, shape of building, color, walls etc)\n",
        "  - Either use supervised learning:\n",
        "    - for these individual features have a huge amount of labeled data \n",
        "  - Or unsupervised\n",
        "    - seperate the most distinct features in these\n",
        "\n",
        "## Dataset\n",
        "> How to get a good dataset?\n",
        "\n",
        "#### Google Maps Street View\n",
        "\n",
        "#### Scrape from Zillow, HotPads, Funda, etc.\n",
        "\n",
        "### Labeling\n",
        "\n",
        "## Things to test\n",
        "Try running simple algorithm on lots of data and see if it picked up on good catagories through **unsupervised learning**.\n",
        "\n",
        "## Roadmap\n",
        "\n",
        "### Proof of concept\n",
        "1. First do some testing, find 10 popular architecture styles\n",
        "2. Find at least 100 images of those architecture styles\n",
        "  - check pintrest\n",
        "  - google images\n",
        "  - zillow\n",
        "  - hotpads\n",
        "  - maybe use cgi to generate houses\n",
        "  - [cool house concepts](https://coolhouseconcepts.com/house-plans/3-bedroom-bungalow-house-plan/)\n",
        "  - check [houseplans.com](https://www.houseplans.com/collection/design-styles)\n",
        "3. Train with labels\n",
        "4. Put on server\n",
        "5. Make api and do some demoing with test frontend\n",
        "\n",
        "### Do the stuff above\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF2uoueW-UP-",
        "colab_type": "text"
      },
      "source": [
        "# Links to possibly intereting things\n",
        "\n",
        "* [Darknet (fast and accurate object / feature detection)](https://github.com/AlexeyAB/darknet)\n",
        "* [Tutorial for Darknet implementation](https://www.youtube.com/watch?v=10joRJt39Ns)\n",
        "* [Darnet Tensorflow implementation)[https://github.com/wizyoung/YOLOv3_TensorFlow]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPRxxhdqOCUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /tmp/houses\n",
        "!rm -rf /data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtBUA41wEjlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "05cf7f80-13e1-4d03-d2f8-db4b4d96fd79"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget https://github.com/thejawker/architector/raw/master/data/picked_house_plans.zip -P /tmp/houses\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/tmp/houses/picked_house_plans.zip', 'r')\n",
        "zip_ref.extractall('/tmp/houses')\n",
        "zip_ref.close()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-19 16:35:22--  https://github.com/thejawker/architector/raw/master/data/picked_house_plans.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/thejawker/architector/master/data/picked_house_plans.zip [following]\n",
            "--2020-04-19 16:35:23--  https://raw.githubusercontent.com/thejawker/architector/master/data/picked_house_plans.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34595948 (33M) [application/zip]\n",
            "Saving to: ‘/tmp/houses/picked_house_plans.zip’\n",
            "\n",
            "picked_house_plans. 100%[===================>]  32.99M  42.8MB/s    in 0.8s    \n",
            "\n",
            "2020-04-19 16:35:24 (42.8 MB/s) - ‘/tmp/houses/picked_house_plans.zip’ saved [34595948/34595948]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U0TF0xdLNLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4359d65d-5466-419f-df64-c9dda2ee2f67"
      },
      "source": [
        "# Prepare the data\n",
        "\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "training_split = 0.7\n",
        "\n",
        "styles = [\n",
        "    'contemporary',\n",
        "    'craftsman'\n",
        "]\n",
        "\n",
        "def copy_dataset(images, split, style):\n",
        "  split_at = int(len(images) * split)\n",
        "  dataset = {\n",
        "    'training': images[:split_at],\n",
        "    'testing': images[split_at:]\n",
        "  }\n",
        "\n",
        "  for (directory, images) in dataset.items():\n",
        "    print(directory)\n",
        "    print(images)\n",
        "    for image in images:\n",
        "      source = '/tmp/houses/picked_house_plans/{}/{}'.format(style, image)\n",
        "      dest = '/data/{}/{}/{}'.format(directory, style, image)\n",
        "      copyfile(source, dest)\n",
        "      \n",
        "\n",
        "for style in styles:\n",
        "  try:\n",
        "    os.makedirs('/data/training/{}'.format(style))\n",
        "    os.makedirs('/data/testing/{}'.format(style))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "for style in styles:\n",
        "  images = os.listdir('/tmp/houses/picked_house_plans/{}'.format(style))\n",
        "  copy_dataset(images, training_split, style)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "['contemporary-1-9.jpg', 'contemporary-2-4.jpg', 'contemporary-12-2.jpg', 'contemporary-23-6.jpg', 'contemporary-11-3.jpg', 'contemporary-1-7.jpg', 'contemporary-1-2.jpg', 'contemporary-18-2.jpg', 'contemporary-9-2.jpg', 'contemporary-11-5.jpg', 'contemporary-6-3.jpg', 'contemporary-13-5.jpg', 'contemporary-11-2.jpg', 'contemporary-8-2.jpg', 'contemporary-9-5.jpg', 'contemporary-1-6.jpg', 'contemporary-3-2.jpg', 'contemporary-28-2.jpg', 'contemporary-22-2.jpg', 'contemporary-18-1.jpg', 'contemporary-19-5.jpg', 'contemporary-29-1.jpg', 'contemporary-28-6.jpg', 'contemporary-2-1.jpg', 'contemporary-4-4.jpg', 'contemporary-9-4.jpg', 'contemporary-4-2.jpg', 'contemporary-7-1.jpg', 'contemporary-24-4.png', 'contemporary-28-4.jpg', 'contemporary-1-10.jpg', 'contemporary-11-4.jpg', 'contemporary-22-15.jpg', 'contemporary-18-4.jpg', 'contemporary-9-3.jpg', 'contemporary-15-3.jpg', 'contemporary-13-4.jpg', 'contemporary-24-1.png', 'contemporary-6-1.jpg', 'contemporary-28-1.jpg', 'contemporary-20-1.jpg', 'contemporary-17-2.jpg', 'contemporary-28-5.jpg', 'contemporary-1-11.jpg', 'contemporary-16-1.jpg', 'contemporary-10-2.jpg', 'contemporary-26-1.jpg', 'contemporary-18-3.jpg', 'contemporary-5-2.jpg', 'contemporary-2-3.jpg', 'contemporary-26-2.jpg', 'contemporary-18-5.jpg', 'contemporary-19-3.jpg', 'contemporary-30-2.jpg', 'contemporary-1-1.jpg', 'contemporary-20-2.jpg', 'contemporary-1-5.jpg', 'contemporary-22-3.jpg', 'contemporary-21-2.jpg', 'contemporary-3-5.jpg', 'contemporary-14-2.jpg', 'contemporary-12-1.jpg', 'contemporary-26-3.jpg', 'contemporary-13-8.jpg', 'contemporary-25-2.jpg', 'contemporary-5-1.jpg', 'contemporary-23-2.jpg', 'contemporary-7-2.jpg', 'contemporary-13-1.jpg', 'contemporary-6-2.jpg', 'contemporary-6-4.jpg', 'contemporary-17-1.jpg', 'contemporary-1-3.jpg', 'contemporary-2-2.jpg', 'contemporary-3-3.jpg', 'contemporary-27-1.jpg', 'contemporary-13-6.jpg', 'contemporary-4-1.jpg', 'contemporary-21-1.jpg']\n",
            "testing\n",
            "['contemporary-19-6.jpg', 'contemporary-8-1.jpg', 'contemporary-22-1.jpg', 'contemporary-13-7.jpg', 'contemporary-8-3.jpg', 'contemporary-30-1.jpg', 'contemporary-14-1.jpg', 'contemporary-19-1.jpg', 'contemporary-8-6.jpg', 'contemporary-1-8.jpg', 'contemporary-19-2.jpg', 'contemporary-1-4.jpg', 'contemporary-9-1.jpg', 'contemporary-8-4.jpg', 'contemporary-24-2.png', '.DS_Store', 'contemporary-23-1.jpg', 'contemporary-6-10.jpg', 'contemporary-4-3.jpg', 'contemporary-8-5.jpg', 'contemporary-11-1.jpg', 'contemporary-13-3.jpg', 'contemporary-17-3.jpg', 'contemporary-23-3.jpg', 'contemporary-25-1.jpg', 'contemporary-28-3.jpg', 'contemporary-15-1.jpg', 'contemporary-10-1.jpg', 'contemporary-15-2.jpg', 'contemporary-13-2.jpg', 'contemporary-2-5.jpg', 'contemporary-3-4.jpg', 'contemporary-3-1.jpg', 'contemporary-24-3.png']\n",
            "training\n",
            "['craftsman-26-3.jpg', 'craftsman-27-31.jpg', 'craftsman-27-32.jpg', 'craftsman-1-8.jpg', 'craftsman-27-2.jpg', 'craftsman-13-4.jpg', 'craftsman-15-5.jpg', 'craftsman-7-4.jpg', 'craftsman-28-1.jpg', 'craftsman-16-1.jpg', 'craftsman-6-2.jpg', 'craftsman-3-9.jpg', 'craftsman-9-2.jpg', 'craftsman-24-1.jpg', 'craftsman-18-2.jpg', 'craftsman-18-1.jpg', 'craftsman-13-5.jpg', 'craftsman-4-28.jpg', 'craftsman-7-5.jpg', 'craftsman-14-1.jpg', 'craftsman-14-2.jpg', 'craftsman-27-4.jpg', 'craftsman-7-6.jpg', 'craftsman-13-22.jpg', 'craftsman-1-5.jpg', 'craftsman-4-27.jpg', 'craftsman-3-10.jpg', 'craftsman-30-2.jpg', 'craftsman-27-8.jpg', 'craftsman-13-3.jpg', 'craftsman-27-34.jpg', 'craftsman-12-1.jpg', 'craftsman-12-2.jpg', 'craftsman-2-2.jpg', 'craftsman-15-1.jpg', 'craftsman-3-7.jpg', 'craftsman-29-1.jpg', 'craftsman-3-5.jpg', 'craftsman-1-2.jpg', 'craftsman-18-17.jpg', 'craftsman-4-3.jpg', 'craftsman-3-2.jpg', 'craftsman-3-8.jpg', 'craftsman-26-5.jpg', 'craftsman-27-7.jpg', 'craftsman-27-33.jpg', 'craftsman-4-1.jpg', 'craftsman-13-1.jpg', 'craftsman-27-36.jpg', 'craftsman-3-1.jpg', 'craftsman-3-4.jpg', 'craftsman-21-2.jpg', 'craftsman-8-6.jpg', 'craftsman-15-4.jpg', 'craftsman-26-2.jpg', 'craftsman-14-61.jpg', 'craftsman-22-1.jpg', 'craftsman-13-2.jpg', 'craftsman-9-14.jpg', 'craftsman-3-6.jpg', 'craftsman-10-1.jpg', 'craftsman-26-1.jpg', 'craftsman-10-3.jpg', 'craftsman-27-35.jpg', 'craftsman-4-2.jpg', 'craftsman-13-21.jpg', 'craftsman-6-1.jpg', 'craftsman-27-1.jpg', 'craftsman-26-6.jpg', 'craftsman-27-5.jpg', 'craftsman-25-2.jpg', 'craftsman-15-3.jpg', 'craftsman-1-1.jpg', 'craftsman-7-2.jpg', 'craftsman-27-6.jpg', 'craftsman-7-1.jpg', 'craftsman-15-2.jpg', 'craftsman-4-23.jpg', 'craftsman-27-3.jpg', 'craftsman-4-25.jpg', 'craftsman-8-3.jpg', 'craftsman-21-11.jpg', 'craftsman-21-12.jpg', 'craftsman-3-3.jpg']\n",
            "testing\n",
            "['craftsman-8-1.jpg', 'craftsman-9-15.jpg', 'craftsman-4-30.jpg', 'craftsman-25-1.jpg', 'craftsman-8-5.jpg', 'craftsman-4-26.jpg', 'craftsman-7-3.jpg', 'craftsman-27-30.jpg', 'craftsman-4-4.jpg', 'craftsman-1-3.jpg', 'craftsman-21-13.jpg', '.DS_Store', 'craftsman-19-1.jpg', 'craftsman-2-1.jpg', 'craftsman-1-4.jpg', 'craftsman-30-1.jpg', 'craftsman-1-6.jpg', 'craftsman-8-4.jpg', 'craftsman-4-24.jpg', 'craftsman-10-4.jpg', 'craftsman-17-1.jpg', 'craftsman-8-2.jpg', 'craftsman-10-2.jpg', 'craftsman-5-1.jpg', 'craftsman-13-6.jpg', 'craftsman-21-1.jpg', 'craftsman-9-16.jpg', 'craftsman-9-1.jpg', 'craftsman-14-28.jpg', 'craftsman-1-7.jpg', 'craftsman-19-2.jpg', 'craftsman-20-1.jpg', 'craftsman-23-1.jpg', 'craftsman-14-27.jpg', 'craftsman-26-4.jpg', 'craftsman-11-1.jpg', 'craftsman-4-29.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGaQdzTMPT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "d47edad5-7440-41f8-d7c0-a481d9c8d38e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    Conv2D(16, (3, 3), input_shape=(180, 120, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(32, (3, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3)),\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(256),\n",
        "    Dense(len(styles), activation='softmax'),\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 178, 118, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 89, 59, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 87, 57, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 43, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 41, 26, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 20, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 18, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 25344)             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1024)              25953280  \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 26,707,362\n",
            "Trainable params: 26,707,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krjy6HrlUDL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "94f0059f-843d-48e2-af60-f2b464c9898d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 30\n",
        "\n",
        "TRAINING_DIR = '/data/training'\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                    batch_size=batch_size,\n",
        "                                                    target_size=(180, 120))\n",
        "\n",
        "VALIDATION_DIR = '/data/testing'\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              target_size=(180, 120))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 163 images belonging to 2 classes.\n",
            "Found 69 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzjFPlzyVzTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "27b7a60a-7146-44e8-9a40-86537317b7db"
      },
      "source": [
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=200,\n",
        "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_steps=STEP_SIZE_VALID,\n",
        "                              validation_data=validation_generator)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.6524 - acc: 0.5667"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-661c5e06bdb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                               validation_data=validation_generator)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWAM_t_1WbUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PLOT LOSS AND ACCURACY\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "\n",
        "\n",
        "plt.title('Training and validation loss')\n",
        "\n",
        "# Desired output. Charts with training and validation metrics. No crash :)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}